<!DOCTYPE html>
<html>

<head>
<% include templates/head %>
<link rel='stylesheet' href='stylesheets/case.css' type='text/css'>
</head>

<body class='case-study' id="dais-case">
	<div class='hero' id='dais'>
		<div class='intro row'>
			<div class='large-12 large-centered columns'>
				<div class="back"><a href="http://elainezhou.me"><img src="images/logoWhite.png"></a></div>
				<div class="intro-text">
					<h1>Dais</h1>
					<h2>Live presentation aid and analytics.</h2>
					<h3>Google Glass, HTML/CSS/Javascript | 2014</h3>
				</div>
			</div>
		</div>
	</div>
	<div class='case-content row'>
		<div class='large-12 large-centered columns'>
			<section id='stats'>
				<div class="large-10 small-12 large-centered columns">
					<div class="row">
						<div class="large-7 columns">
							<h4>Roles</h4>
							<p>Field Research, Interviews, Interaction Design, User Testing, Prototyping, Data Visualization</p>
						</div>
						<div class="large-5 columns">
							<h4>Context</h4>
							<p>
							Coursework at Stanford<br />
							</p>
						</div>
					</div>

				</div>
			</section>

			<section id='overview'>
				<div class="large-10 small-12 large-centered columns">
					<h3>Present Better</h3>

					<p>Why is public speaking so difficult? Many presentation aids don't teach nor reinforce good speaking techniques; instead, they focus on making the presenter’s notes more accessible or slide changes easier. Dais aims to help novice oral presenters to receive real-time feedback and present aggregate trends of their performances.
					</p>
					
					<div class='video-wrapper center'>
						<iframe width="560" height="315" src="https://www.youtube.com/embed/x7TjLtVKj3k" frameborder="0" allowfullscreen></iframe>
					</div>
				</div>
			</section>

			<section id='needfinding'>
				<div class="large-10 small-12 large-centered columns">
					<h3>Needfinding</h3>
					<p>We observed and talked with novice speakers and interviewed two professional oral communicator lecturers - <a href='https://www.linkedin.com/pub/matt-vassar/28/196/798'>Matt Vassar</a> and <a href='http://vpuewh-public.stanford.edu/viewprofile.jsp?sunetid=sohui&usetype=P&userid=PWR'>Sohui Lee</a>. Conversations with and observations of novice speakers surfaced that too much immediate feedback is overwhelming. Video recording, the most common presentation debugging method, is typically ineffective due to its awkwardness. Here, we narrowed our problem space: non-professional oral presenters need a way to receive small amounts of immediate and intimate feedback and then later analyze details of their performances.</p>

					<blockquote>Time and sound [are] so warped when you’re presenting. Ten minutes can feel like forever! <footer>- Dr. Sohui Lee</footer></blockquote>

					<p>We decided to focus on two main pillars of good presentations: <em>balanced visual spread</em> and <em>consistent speaking volume</em>. We could then utilize the inherent strengths of wearables: the ability to analyze a constant stream of real-world sensor data to interpret and classify user behaviors.  On the flipside, this proved to be an interaction challenges - providing the right amount of information at the right time in microinteractions.
					</p>
				</div>

			</section>

			<section id='glass'>
				<div class="large-10 small-12 large-centered columns">
					<h3>Google Glass Prototyping</h3>

					<p>By taking initial reference points from the accelerometer and microphone, Dais monitors where the user is looking and their volume. It then provides immediate feedback on eye-contact spread and volume. For example, if a user spends a disproportionate large amount of time looking at audience members to his/her left, Dais displays a subtle suggestion to look elsewhere. If the user looks far out of the range of the audience, Dais assumes the user has turned their head away from the audience to read off their own slides telling the user to face forward. We prototyped some screens for immediate visual and aural feedback. 
					</p>
					<figure>
						<img src='images/dais/directionalView.png' />
						<img src='images/dais/spatialView.png' />
						<figcaption>We prototyped two different types of feedback cues for Glass – directional (top) and spatial (bottom).</figcaption>
					</figure>

					<p>We also prototyped some visualizations summarizing an entire presentation - heat map of head position to model visual spread and a line graph charting the progression of the speaker’s volume. However, it was immediately clear from user feedback that this information was too dense and unnecessary for Glass' intended use case.
					</p>

					<figure>
						<!-- <img src='images/dais/glassAural.png' /> -->
						<img src='images/dais/glassVisual.png' />
						<figcaption>We tried displaying these analytics on Glass, but they were incomprehensible.</figcaption>
					</figure>
				</div>

			</section>

			<section id='web'>
				<div class="large-10 small-12 large-centered columns">
					<h3>Web App Component</h3>
					<p>Taking these insights into account, we built a real-time app platform on which the presenter can view important metrics from any previously given presentationa. Since the goal of post-presentation evaluation is pinpointing and correcting errors made during the presentation, the analytics were designed with the intention of highlighting errors made. The heat map highlights the user's tendency to stare at any single point, and the volume line graph indicates the user's volume relative to a desired speaking level. The web app also aggregate all presentations over time in one dashboard.
					</p>

					<figure>
						<img src='images/dais/webapp.png' />
						<figcaption>The web app displays aggregate analytics after the presentation has ended.</figcaption>
					</figure>
				</div>

			</section>
		</div>
	</div>
</div>
<footer class='row'>
	<div class='large-12 large-centered columns light center'>
		<hr class='center'></hr>
		 <p>Elaine Zhou • 2016</p>
	</div>
</footer>

<% include templates/scripts %>

</body>
</html>